<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Veille Technologique : Grands Mod√®les de Langage</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    /* Global styles */
    body {
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      background-color: #f4f6f9;
      color: #1f2937;
      scroll-behavior: smooth;
    }
    header {
      background: linear-gradient(to right, #1e3a8a, #3b82f6);
      color: white;
      padding: 2rem 1rem;
      text-align: center;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    }
    header h1 {
      margin: 0;
      font-size: 2rem;
    }
    header p {
      margin: 0.5rem 0 0;
      font-size: 1.1rem;
    }
    /* Navigation Bar */
    nav {
      background: #fff;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    nav ul {
      display: flex;
      justify-content: center;
      list-style: none;
      margin: 0;
      padding: 0.5rem 0;
    }
    nav ul li {
      margin: 0 1rem;
    }
    nav ul li a {
      color: #1e3a8a;
      text-decoration: none;
      font-weight: 600;
      padding: 0.5rem 1rem;
      transition: background 0.3s, color 0.3s;
      border-radius: 5px;
    }
    nav ul li a:hover,
    nav ul li a.active {
      background: #1e3a8a;
      color: #fff;
    }
    /* Section styling */
    main {
      margin-top: 1rem;
    }
    section {
      padding: 3rem 10%;
    }
    .card {
      background: white;
      border-radius: 20px;
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.1);
      padding: 2rem;
      margin-bottom: 3rem;
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    .card:hover {
      transform: translateY(-5px);
      box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
    }
    h1, h2, h3, h4 {
      color: #1e3a8a;
      font-weight: 800;
    }
    h2 { margin-bottom: 1rem; }
    img {
      max-width: 100%;
      border-radius: 12px;
      margin-top: 1.5rem;
      box-shadow: 0 2px 12px rgba(0, 0, 0, 0.05);
    }
    ul {
      padding-left: 1.5rem;
    }
    ul li { margin-bottom: 0.5rem; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
    }
    table th,
    table td {
      padding: 12px;
      border: 1px solid #ddd;
      text-align: left;
    }
    table thead { background-color: #1e3a8a; color: white; }
    /* Footer */
    footer {
      background-color: #1e3a8a;
      color: white;
      text-align: center;
      padding: 1rem;
      margin-top: 2rem;
    }
    /* Responsive adjustments */
    @media(max-width: 768px) {
      section { padding: 2rem 5%; }
      nav ul li { margin: 0 0.5rem; }
    }
  </style>
</head>
<body>

  <header>
    <h1>Veille Technologique : L'√©volution des Grands Mod√®les de Langage (LLM)</h1>
    <p>Scalabilit√©, Optimisation et Applications sp√©cialis√©es</p>
  </header>

  <!-- Barre de navigation interactive -->
  <nav>
    <ul>
      <li><a href="#introduction" class="nav-link active">Introduction & Histoire</a></li>
      <li><a href="#architecture" class="nav-link">Architecture</a></li>
      <li><a href="#fonctionnement" class="nav-link">Fonctionnement</a></li>
      <li><a href="#optimisation" class="nav-link">Optimisation</a></li>
      <li><a href="#tendances" class="nav-link">Tendances</a></li>
      <li><a href="#outils" class="nav-link">Outils de Veille</a></li>
      <li><a href="#bibliographie" class="nav-link">Ressources Bibliographiques</a></li>
    </ul>
  </nav>

  <!-- Contenu principal -->
  <main>
    <!-- Section 1: Introduction & Histoire -->
    <section id="introduction">
      <div class="card">
        <h2>1. Introduction & Histoire des LLM</h2>
        <p>Les grands mod√®les de langage (LLM) sont des r√©seaux neuronaux entra√Æn√©s sur de vastes quantit√©s de texte afin d‚Äôaccomplir une large vari√©t√© de t√¢ches linguistiques : traduction, r√©sum√©, g√©n√©ration de texte, question-r√©ponse, etc. Leur √©volution est intimement li√©e aux avanc√©es des architectures de r√©seaux, √† la croissance exponentielle des donn√©es disponibles, et √† l‚Äôaugmentation des capacit√©s de calcul.</p>
        <p>Tout a commenc√© avec des mod√®les classiques comme Word2Vec ou GloVe, avant l'arriv√©e r√©volutionnaire des Transformers en 2017 (par Vaswani et al.). Ensuite, des mod√®les comme BERT (2018) et GPT-2/3/4 ont marqu√© des √©tapes majeures dans la compr√©hension contextuelle et la g√©n√©ration fluide de texte. Aujourd'hui, les LLMs sont utilis√©s dans des domaines vari√©s : assistants virtuels, diagnostic m√©dical, juridique, finance, cr√©ation artistique, et plus encore.</p>
        <p>Voici une frise chronologique illustrant les grandes √©tapes de l‚Äô√©volution des LLM :</p>
        <img src="timeline_llm.png" alt="Frise chronologique des LLM">
      </div>
    </section>

    <!-- Section 2: Architecture -->
    <section id="architecture">
      <div class="card">
        <h2>2. Architecture des LLM</h2>
        <p>Les architectures des LLM reposent toutes sur le Transformer, introduit par Vaswani et al. en 2017. Celui-ci remplace les r√©seaux r√©currents par un m√©canisme d'attention multi-t√™te qui traite les s√©quences en parall√®le, permettant ainsi une grande efficacit√©.</p>
        <p>Un Transformer est compos√© de deux blocs principaux :</p>
        <ul>
          <li><strong>Encodeur :</strong> utile pour les t√¢ches de compr√©hension (ex : BERT)</li>
          <li><strong>D√©codeur :</strong> utilis√© pour la g√©n√©ration de texte (ex : GPT)</li>
        </ul>
        <p><strong>BERT</strong> (Google, 2018) est un mod√®le bas√© uniquement sur l'encodeur. Il apprend de fa√ßon bidirectionnelle, c‚Äôest-√†-dire qu‚Äôil tient compte du contexte √† gauche et √† droite d‚Äôun mot. Il excelle dans les t√¢ches de compr√©hension de texte comme la classification ou les questions-r√©ponses.</p>
        <p><strong>GPT</strong> (OpenAI) est bas√© uniquement sur le d√©codeur. Il g√©n√®re du texte mot par mot en tenant compte uniquement du contexte pr√©c√©dent (auto-r√©gressif). Il est con√ßu pour produire du texte fluide et coh√©rent.</p>
        <p>Voici un sch√©ma simplifi√© des deux architectures :</p>
        <div style="display: flex; flex-wrap: wrap; justify-content: space-around; gap: 2rem;">
          <div style="flex: 1; min-width: 280px; background: #f0f4ff; padding: 1rem; border-radius: 12px;">
            <h4 style="text-align: center; color: #1e3a8a;">BERT (Encodeur uniquement)</h4>
            <ul>
              <li>Entr√©e ‚Üí Embedding ‚Üí Encodeur √ó N ‚Üí Sortie</li>
              <li>Contextualisation bidirectionnelle</li>
              <li>T√¢ches de compr√©hension</li>
            </ul>
          </div>
          <div style="flex: 1; min-width: 280px; background: #fff3e0; padding: 1rem; border-radius: 12px;">
            <h4 style="text-align: center; color: #c2410c;">GPT (D√©codeur uniquement)</h4>
            <ul>
              <li>Entr√©e ‚Üí Embedding ‚Üí D√©codeur √ó N ‚Üí Texte g√©n√©r√©</li>
              <li>Contextualisation unidirectionnelle (gauche ‚Üí droite)</li>
              <li>T√¢ches de g√©n√©ration</li>
            </ul>
          </div>
        </div>
        <p>Certains mod√®les modernes comme <strong>T5</strong> ou <strong>PaLM</strong> utilisent l'architecture compl√®te encodeur-d√©codeur pour transformer toutes les t√¢ches NLP en une t√¢che de g√©n√©ration de texte.</p>
        <p>Voici une illustration comparative de l'architecture Transformer utilis√©e par BERT et GPT :</p>
        <img src="26.png" alt="Architecture Transformer - BERT vs GPT">
      </div>
    </section>

    <!-- Section 3: Fonctionnement -->
    <section id="fonctionnement">
      <div class="card">
        <h2>3. Fonctionnement des LLM : Pr√©-entra√Ænement ‚Üí Fine-tuning ‚Üí RLHF ‚Üí D√©ploiement</h2>
        <p>Le fonctionnement des grands mod√®les de langage repose sur plusieurs phases successives et compl√©mentaires :</p>
        <h3>Pr√©-entra√Ænement</h3>
        <p>Le LLM est entra√Æn√© √† pr√©dire des mots dans d‚Äôimmenses corpus de texte. Il s‚Äôagit d‚Äôun apprentissage auto-supervis√© sur des t√¢ches comme la mod√©lisation de langage masqu√© (BERT) ou la pr√©diction du token suivant (GPT).</p>
        <ul>
          <li>Corpus : Web, livres, Wikip√©dia, code source‚Ä¶</li>
          <li>Objectif : Apprendre les patterns du langage humain.</li>
          <li>Mod√®le : Initialis√© avec des param√®tres al√©atoires puis optimis√© sur des milliards de jetons.</li>
        </ul>
        <h3>Fine-tuning (r√©glage fin)</h3>
        <p>Le mod√®le est ensuite adapt√© √† des t√¢ches sp√©cifiques (ex. : r√©sum√©, Q&R, classification) via un ensemble de donn√©es plus petit et plus cibl√©. Cela am√©liore sa performance sur des cas concrets.</p>
        <ul>
          <li>Approche : Supervised Fine-Tuning (SFT)</li>
          <li>Donn√©es : paires question-r√©ponse, exemples annot√©s</li>
          <li>Exemple : InstructGPT</li>
        </ul>
        <h3>RLHF (Reinforcement Learning with Human Feedback)</h3>
        <p>Pour rendre les r√©ponses plus utiles, le LLM est affin√© via l‚Äôapprentissage par renforcement bas√© sur des pr√©f√©rences humaines. Il apprend √† classer et am√©liorer ses propres sorties.</p>
        <ul>
          <li>Phase 1 : Collecte de pr√©f√©rences humaines</li>
          <li>Phase 2 : Apprentissage d‚Äôun mod√®le de r√©compense</li>
          <li>Phase 3 : Optimisation par PPO (Policy Optimization)</li>
        </ul>
        <h3>D√©ploiement</h3>
        <p>Une fois entra√Æn√©, le mod√®le est d√©ploy√© via une API, sur le cloud ou localement. Il peut √™tre int√©gr√© √† des applications (chatbots, assistants, syst√®mes de recommandation...)</p>
        <h3>Techniques compl√©mentaires</h3>
        <ul>
          <li><strong>Prompt engineering</strong> : formuler intelligemment les requ√™tes pour guider le comportement du mod√®le</li>
          <li><strong>RAG (Retrieval-Augmented Generation)</strong> : enrichir les prompts avec de la connaissance externe (base vectorielle)</li>
          <li><strong>Continual Pretraining</strong> : affiner un mod√®le avec de nouvelles donn√©es non annot√©es</li>
        </ul>
        <h3>Tableau comparatif des approches</h3>
        <div style="overflow-x: auto;">
          <table>
            <thead>
              <tr>
                <th>M√©thode</th>
                <th>But</th>
                <th>Donn√©es requises</th>
                <th>Avantages</th>
                <th>Limites</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Pr√©-entra√Ænement</td>
                <td>Mod√®le g√©n√©raliste</td>
                <td>Corpus massif (non annot√©)</td>
                <td>Apprentissage profond des patterns</td>
                <td>Co√ªteux, long, demande GPU</td>
              </tr>
              <tr>
                <td>Fine-tuning</td>
                <td>Sp√©cialisation</td>
                <td>Corpus annot√© (petit)</td>
                <td>Haute performance sur t√¢che pr√©cise</td>
                <td>Peut biaiser le mod√®le</td>
              </tr>
              <tr>
                <td>RLHF</td>
                <td>Alignement avec l'humain</td>
                <td>Retours humains</td>
                <td>R√©ponses plus utiles</td>
                <td>Complexit√© d‚Äôimpl√©mentation</td>
              </tr>
              <tr>
                <td>Prompt Engineering</td>
                <td>Optimisation √† la vol√©e</td>
                <td>Aucune</td>
                <td>Simple, rapide</td>
                <td>Moins robuste</td>
              </tr>
              <tr>
                <td>RAG</td>
                <td>Connaissances dynamiques</td>
                <td>Base externe (vectorielle)</td>
                <td>Informations √† jour</td>
                <td>Co√ªt d‚Äôinf√©rence</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Ces √©tapes combin√©es permettent aux LLM d‚Äô√™tre √† la fois puissants, adaptables et efficaces pour une multitude d‚Äôapplications.</p>
      </div>
    </section>

    <!-- Section 4: Optimisation -->
    <section id="optimisation">
      <div class="card">
        <h2>4. Optimisation des LLMs : M√©thodes Avanc√©es</h2>
        <p>Les grands mod√®les de langage n√©cessitent des techniques avanc√©es pour r√©duire leur co√ªt d'entra√Ænement et d'inf√©rence tout en maintenant leur efficacit√©. Voici deux m√©thodes majeures :</p>
        
        <h3>üîπ LoRA (Low-Rank Adaptation)</h3>
        <p>LoRA est une approche qui permet d‚Äôadapter un mod√®le existant en gelant ses poids d'origine et en n'entra√Ænant que des matrices de faible rang suppl√©mentaires. Cela r√©duit significativement le nombre de param√®tres ajust√©s.</p>
        <p>LoRA repose sur une factorisation de matrice o√π une matrice de poids pr√©-entra√Æn√©e <code>W</code> est d√©compos√©e en deux matrices de plus faible rang <code>A</code> et <code>B</code> telles que :</p>
        <p style="text-align: center; font-style: italic; font-size: 1.2rem;">ŒîW = A √ó B<sup>T</sup></p>
        <p>Cela permet de minimiser le nombre de param√®tres ajust√©s tout en conservant la richesse du mod√®le original.</p>
        <ul>
          <li><strong>Avantages :</strong> R√©duction du co√ªt de calcul et de m√©moire.</li>
          <li><strong>Limites :</strong> Moins performant pour certaines t√¢ches complexes n√©cessitant une mise √† jour globale des poids.</li>
        </ul>
        <img src="27.png" alt="Illustration de la m√©thode LoRA">
        
        <h3>üîπ MoE (Mixture of Experts)</h3>
        <p>MoE repose sur l'activation s√©lective de sous-mod√®les appel√©s "experts" pour traiter diff√©rentes parties d‚Äôune requ√™te, ce qui permet d'utiliser de grands mod√®les tout en limitant la charge de calcul.</p>
        <p>Dans MoE, un m√©canisme de gating <code>G(x)</code> s√©lectionne dynamiquement un sous-ensemble d'experts <code>E<sub>i</sub></code>, chaque expert ayant son propre ensemble de param√®tres <code>W<sub>i</sub></code> :</p>
        <p style="text-align: center; font-style: italic; font-size: 1.2rem;">y = ‚àë<sub>i=1</sub><sup>N</sup> G<sub>i</sub>(x) √ó E<sub>i</sub>(x)</p>
        <p>o√π <code>G<sub>i</sub>(x)</code> est une fonction d'activation qui attribue un poids √† chaque expert.</p>
        <ul>
          <li><strong>Avantages :</strong> Scalabilit√© accrue et flexibilit√©.</li>
          <li><strong>Limites :</strong> Complexit√© accrue dans la gestion des experts et latence potentielle due au routage dynamique.</li>
        </ul>
        <img src="28.png" alt="Sch√©ma du Mixture of Experts">
        
        <h3>Comparaison des m√©thodes</h3>
        <table>
          <thead>
            <tr>
              <th>M√©thode</th>
              <th>Objectif</th>
              <th>Avantages</th>
              <th>Limites</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>LoRA</td>
              <td>Fine-tuning rapide et peu co√ªteux</td>
              <td>Faible consommation de m√©moire</td>
              <td>Moins adapt√© aux t√¢ches n√©cessitant une refonte du mod√®le</td>
            </tr>
            <tr>
              <td>MoE</td>
              <td>Modularit√© et gestion de grands mod√®les</td>
              <td>Scalabilit√© et optimisation de la charge</td>
              <td>Complexit√© de gestion et latence possible</td>
            </tr>
          </tbody>
        </table>
        <p>Ces deux approches sont compl√©mentaires et peuvent √™tre combin√©es pour am√©liorer l'efficacit√© des mod√®les de langage.</p>
      </div>
    </section>

    <!-- Section 5: Tendances -->
    <section id="tendances">
      <div class="card">
        <h2>5. Tendances Actuelles et Comparaison des Mod√®les de Langage</h2>
        <p>Les grands mod√®les de langage (LLM) √©voluent rapidement, avec des avanc√©es en mati√®re d'efficacit√©, de scalabilit√© et de sp√©cialisation. Voici quelques tendances majeures :</p>
        
        <h3>üîπ Tendance 1 : Mod√®les Open-Source vs Propri√©taires</h3>
        <p>Les mod√®les de langage se divisent aujourd‚Äôhui en deux grandes cat√©gories :</p>
        <ul>
          <li><strong>Mod√®les propri√©taires</strong> : D√©velopp√©s par des entreprises comme OpenAI (GPT-4), Google DeepMind (Gemini), ou Anthropic (Claude), ces mod√®les sont g√©n√©ralement ferm√©s et optimis√©s pour des performances de pointe. Toutefois, leur opacit√© en mati√®re d'entra√Ænement et de fonctionnement pose des d√©fis en termes de transparence et de contr√¥le.</li>
          <li><strong>Mod√®les open-source</strong> : Des alternatives comme LLaMA 2 (Meta), Falcon (Technology Innovation Institute) et DeepSeek (DeepSeek AI) permettent aux chercheurs et ing√©nieurs de les modifier et de les adapter √† des besoins sp√©cifiques, favorisant ainsi une plus grande transparence et une adoption plus large.</li>
        </ul>
        
        <h3>üîπ Tendance 2 : Efficacit√© et R√©duction de la Consommation</h3>
        <p>Avec l‚Äôaugmentation exponentielle de la taille des mod√®les, la question de l‚Äôefficacit√© devient centrale. Plusieurs approches sont utilis√©es pour optimiser la consommation √©nerg√©tique et le co√ªt :</p>
        <ul>
          <li><strong>Quantization</strong> : R√©duction de la pr√©cision des poids (ex. FP16, INT8) pour acc√©l√©rer l‚Äôinf√©rence et r√©duire la consommation de m√©moire.</li>
          <li><strong>Distillation</strong> : Extraction des connaissances d‚Äôun mod√®le volumineux pour entra√Æner un mod√®le plus l√©ger sans perte significative de performance.</li>
          <li><strong>Fine-tuning efficace</strong> : Approches comme LoRA et AdapterFusion permettent d‚Äôadapter un mod√®le √† de nouvelles t√¢ches sans r√©entra√Ænement complet.</li>
        </ul>
        
        <h3>üîπ Tendance 3 : Sp√©cialisation et Fine-Tuning</h3>
        <p>Plut√¥t que d'entra√Æner des mod√®les g√©n√©ralistes, l'industrie se tourne vers des mod√®les sp√©cialis√©s :</p>
        <ul>
          <li><strong>BioGPT</strong> : D√©di√© aux applications biom√©dicales et √† la recherche m√©dicale.</li>
          <li><strong>Codex</strong> : Con√ßu pour la g√©n√©ration et la compl√©tion de code informatique.</li>
          <li><strong>BloombergGPT</strong> : Ax√© sur les domaines de la finance et de l'analyse √©conomique.</li>
          <li><strong>DeepSeek</strong> : Un mod√®le open-source puissant qui se concentre sur la compr√©hension et la g√©n√©ration de texte en plusieurs langues, notamment en chinois et en anglais.</li>
        </ul>
        
        <p>La course √† l‚Äôinnovation dans le domaine des LLMs ne cesse d‚Äôacc√©l√©rer, avec une attention croissante port√©e √† l'efficacit√©, la sp√©cialisation et l‚Äôaccessibilit√© des mod√®les.</p>
      </div>
    </section>

    <!-- Section 6: Outils de Veille -->
    <section id="outils">
      <div class="card">
        <h2>6. Outils de Veille Utilis√©s</h2>
        <h3>Recherche et Collecte d'Informations</h3>
        <ul>
          <li><strong>Google Scholar</strong> : Moteur de recherche acad√©mique permettant d'acc√©der aux articles scientifiques et brevets.</li>
          <li><strong>Arxiv.org &amp; Arxiv-Sanity</strong> : Plateforme incontournable pour les pr√©publications en intelligence artificielle et machine learning.</li>
          <li><strong>Semantic Scholar</strong> : Outil avanc√© d'indexation des articles scientifiques avec des fonctionnalit√©s d'analyse s√©mantique.</li>
        </ul>
        <h3>Organisation et Gestion des Sources</h3>
        <ul>
          <li><strong>Zotero</strong> : Outil de gestion bibliographique permettant de stocker, organiser et citer des articles.</li>
          <li><strong>Mendeley</strong> : Alternative √† Zotero pour la gestion et l'annotation de r√©f√©rences scientifiques.</li>
        </ul>
        <h3>üì° Veille en Temps R√©el</h3>
        <ul>
          <li><strong>Feedly</strong> : Agr√©gateur de flux RSS permettant de suivre les publications r√©centes.</li>
          <li><strong>Twitter/X</strong> : R√©seau social o√π chercheurs et laboratoires publient des avanc√©es en temps r√©el.</li>
          <li><strong>LinkedIn</strong> : Plateforme professionnelle utile pour suivre les innovations et conf√©rences en IA.</li>
          <li><strong>Hugging Face Forums</strong> : Communaut√© d√©di√©e aux LLMs, avec des discussions sur les derniers mod√®les et optimisations.</li>
          <li><strong>PapersWithCode</strong> : Base de donn√©es associant articles scientifiques et impl√©mentations open-source.</li>
        </ul>
      </div>
    </section>

    <!-- Section 7: Ressources Bibliographiques -->
    <section id="bibliographie">
      <div class="card">
        <h2>7. Ressources Bibliographiques</h2>
        <p>Les r√©f√©rences suivantes regroupent les principales publications scientifiques utilis√©es dans cette veille technologique :</p>
        <h3>üìÑ Articles Scientifiques</h3>
        <ul>
          <li><a href="https://arxiv.org/abs/2106.09685" target="_blank">LoRA: Low-Rank Adaptation of Large Language Models</a> - Hu et al., 2021</li>
          <li><a href="https://arxiv.org/abs/2406.03136" target="_blank">Limitations of LoRA: A Comprehensive Analysis</a> - √âtude des contraintes de LoRA</li>
          <li><a href="https://openreview.net/forum?id=B1ckMDqlg" target="_blank">Mixture of Experts in Transformers</a> - Shazeer et al., 2017</li>
          <li><a href="https://arxiv.org/abs/2202.00580" target="_blank">Scaling Laws for Neural Language Models</a> - Analyse des relations entre taille du mod√®le et performance</li>
        </ul>
        <h3>üìö Plateformes et Outils</h3>
        <ul>
          <li><a href="https://huggingface.co" target="_blank">Hugging Face</a> : Hub collaboratif pour le d√©veloppement de mod√®les IA</li>
          <li><a href="https://paperswithcode.com" target="_blank">Papers with Code</a> : Suivi des derni√®res avanc√©es en IA avec impl√©mentations disponibles</li>
          <li><a href="https://www.semanticscholar.org" target="_blank">Semantic Scholar</a> : Base de donn√©es scientifique avec indexation avanc√©e</li>
        </ul>
      </div>
    </section>
  </main>

  <footer>
    <p>¬© 2025 Mehdi Janati Idrissi ‚Äî Projet de Veille Technologique Centrale Lyon</p>
  </footer>

  <!-- Script pour le scroll fluide et la mise en surbrillance de la section active -->
  <script>
    const navLinks = document.querySelectorAll('.nav-link');

    // Supprime la classe active de tous les liens
    function removeActiveClasses() {
      navLinks.forEach(link => link.classList.remove('active'));
    }

    // Ajoute la classe active lors d'un clic sur un lien
    navLinks.forEach(link => {
      link.addEventListener('click', function() {
        removeActiveClasses();
        this.classList.add('active');
      });
    });

    // Met √† jour le lien actif en fonction de la position de scroll
    window.addEventListener('scroll', () => {
      let fromTop = window.scrollY + 100;
      navLinks.forEach(link => {
        let section = document.querySelector(link.getAttribute('href'));
        if (
          section.offsetTop <= fromTop &&
          section.offsetTop + section.offsetHeight > fromTop
        ) {
          removeActiveClasses();
          link.classList.add('active');
        }
      });
    });
  </script>
</body>
</html>
