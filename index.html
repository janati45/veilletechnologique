<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Veille Technologique : Grands Modèles de Langage</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    /* Global styles */
    body {
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      background-color: #f4f6f9;
      color: #1f2937;
      scroll-behavior: smooth;
    }
    header {
      background: linear-gradient(to right, #1e3a8a, #3b82f6);
      color: white;
      padding: 2rem 1rem;
      text-align: center;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    }
    header h1 {
      margin: 0;
      font-size: 2rem;
    }
    header p {
      margin: 0.5rem 0 0;
      font-size: 1.1rem;
    }
    /* Navigation Bar */
    nav {
      background: #fff;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
      position: sticky;
      top: 0;
      z-index: 1000;
    }
    nav ul {
      display: flex;
      justify-content: center;
      list-style: none;
      margin: 0;
      padding: 0.5rem 0;
    }
    nav ul li {
      margin: 0 1rem;
    }
    nav ul li a {
      color: #1e3a8a;
      text-decoration: none;
      font-weight: 600;
      padding: 0.5rem 1rem;
      transition: background 0.3s, color 0.3s;
      border-radius: 5px;
    }
    nav ul li a:hover,
    nav ul li a.active {
      background: #1e3a8a;
      color: #fff;
    }
    /* Section styling */
    main {
      margin-top: 1rem;
    }
    section {
      padding: 3rem 10%;
    }
    .card {
      background: white;
      border-radius: 20px;
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.1);
      padding: 2rem;
      margin-bottom: 3rem;
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }
    .card:hover {
      transform: translateY(-5px);
      box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
    }
    h1, h2, h3, h4 {
      color: #1e3a8a;
      font-weight: 800;
    }
    h2 { margin-bottom: 1rem; }
    img {
      max-width: 100%;
      border-radius: 12px;
      margin-top: 1.5rem;
      box-shadow: 0 2px 12px rgba(0, 0, 0, 0.05);
    }
    ul {
      padding-left: 1.5rem;
    }
    ul li { margin-bottom: 0.5rem; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
    }
    table th,
    table td {
      padding: 12px;
      border: 1px solid #ddd;
      text-align: left;
    }
    table thead { background-color: #1e3a8a; color: white; }
    /* Footer */
    footer {
      background-color: #1e3a8a;
      color: white;
      text-align: center;
      padding: 1rem;
      margin-top: 2rem;
    }
    /* Responsive adjustments */
    @media(max-width: 768px) {
      section { padding: 2rem 5%; }
      nav ul li { margin: 0 0.5rem; }
    }
  </style>
</head>
<body>

  <header>
    <h1>Veille Technologique : L'évolution des Grands Modèles de Langage (LLM)</h1>
    <p>Scalabilité, Optimisation et Applications spécialisées</p>
  </header>

  <!-- Barre de navigation interactive -->
  <nav>
    <ul>
      <li><a href="#introduction" class="nav-link active">Introduction & Histoire</a></li>
      <li><a href="#architecture" class="nav-link">Architecture</a></li>
      <li><a href="#fonctionnement" class="nav-link">Fonctionnement</a></li>
      <li><a href="#optimisation" class="nav-link">Optimisation</a></li>
      <li><a href="#tendances" class="nav-link">Tendances</a></li>
      <li><a href="#outils" class="nav-link">Outils de Veille</a></li>
      <li><a href="#bibliographie" class="nav-link">Ressources Bibliographiques</a></li>
    </ul>
  </nav>

  <!-- Contenu principal -->
  <main>
    <!-- Section 1: Introduction & Histoire -->
    <section id="introduction">
      <div class="card">
        <h2>1. Introduction & Histoire des LLM</h2>
        <p>Les grands modèles de langage (LLM) sont des réseaux neuronaux entraînés sur de vastes quantités de texte afin d’accomplir une large variété de tâches linguistiques : traduction, résumé, génération de texte, question-réponse, etc. Leur évolution est intimement liée aux avancées des architectures de réseaux, à la croissance exponentielle des données disponibles, et à l’augmentation des capacités de calcul.</p>
        <p>Tout a commencé avec des modèles classiques comme Word2Vec ou GloVe, avant l'arrivée révolutionnaire des Transformers en 2017 (par Vaswani et al.). Ensuite, des modèles comme BERT (2018) et GPT-2/3/4 ont marqué des étapes majeures dans la compréhension contextuelle et la génération fluide de texte. Aujourd'hui, les LLMs sont utilisés dans des domaines variés : assistants virtuels, diagnostic médical, juridique, finance, création artistique, et plus encore.</p>
        <p>Voici une frise chronologique illustrant les grandes étapes de l’évolution des LLM :</p>
        <img src="timeline_llm.png" alt="Frise chronologique des LLM">
      </div>
    </section>

    <!-- Section 2: Architecture -->
    <section id="architecture">
      <div class="card">
        <h2>2. Architecture des LLM</h2>
        <p>Les architectures des LLM reposent toutes sur le Transformer, introduit par Vaswani et al. en 2017. Celui-ci remplace les réseaux récurrents par un mécanisme d'attention multi-tête qui traite les séquences en parallèle, permettant ainsi une grande efficacité.</p>
        <p>Un Transformer est composé de deux blocs principaux :</p>
        <ul>
          <li><strong>Encodeur :</strong> utile pour les tâches de compréhension (ex : BERT)</li>
          <li><strong>Décodeur :</strong> utilisé pour la génération de texte (ex : GPT)</li>
        </ul>
        <p><strong>BERT</strong> (Google, 2018) est un modèle basé uniquement sur l'encodeur. Il apprend de façon bidirectionnelle, c’est-à-dire qu’il tient compte du contexte à gauche et à droite d’un mot. Il excelle dans les tâches de compréhension de texte comme la classification ou les questions-réponses.</p>
        <p><strong>GPT</strong> (OpenAI) est basé uniquement sur le décodeur. Il génère du texte mot par mot en tenant compte uniquement du contexte précédent (auto-régressif). Il est conçu pour produire du texte fluide et cohérent.</p>
        <p>Voici un schéma simplifié des deux architectures :</p>
        <div style="display: flex; flex-wrap: wrap; justify-content: space-around; gap: 2rem;">
          <div style="flex: 1; min-width: 280px; background: #f0f4ff; padding: 1rem; border-radius: 12px;">
            <h4 style="text-align: center; color: #1e3a8a;">BERT (Encodeur uniquement)</h4>
            <ul>
              <li>Entrée → Embedding → Encodeur × N → Sortie</li>
              <li>Contextualisation bidirectionnelle</li>
              <li>Tâches de compréhension</li>
            </ul>
          </div>
          <div style="flex: 1; min-width: 280px; background: #fff3e0; padding: 1rem; border-radius: 12px;">
            <h4 style="text-align: center; color: #c2410c;">GPT (Décodeur uniquement)</h4>
            <ul>
              <li>Entrée → Embedding → Décodeur × N → Texte généré</li>
              <li>Contextualisation unidirectionnelle (gauche → droite)</li>
              <li>Tâches de génération</li>
            </ul>
          </div>
        </div>
        <p>Certains modèles modernes comme <strong>T5</strong> ou <strong>PaLM</strong> utilisent l'architecture complète encodeur-décodeur pour transformer toutes les tâches NLP en une tâche de génération de texte.</p>
        <p>Voici une illustration comparative de l'architecture Transformer utilisée par BERT et GPT :</p>
        <img src="26.png" alt="Architecture Transformer - BERT vs GPT">
      </div>
    </section>

    <!-- Section 3: Fonctionnement -->
    <section id="fonctionnement">
      <div class="card">
        <h2>3. Fonctionnement des LLM : Pré-entraînement → Fine-tuning → RLHF → Déploiement</h2>
        <p>Le fonctionnement des grands modèles de langage repose sur plusieurs phases successives et complémentaires :</p>
        <h3>Pré-entraînement</h3>
        <p>Le LLM est entraîné à prédire des mots dans d’immenses corpus de texte. Il s’agit d’un apprentissage auto-supervisé sur des tâches comme la modélisation de langage masqué (BERT) ou la prédiction du token suivant (GPT).</p>
        <ul>
          <li>Corpus : Web, livres, Wikipédia, code source…</li>
          <li>Objectif : Apprendre les patterns du langage humain.</li>
          <li>Modèle : Initialisé avec des paramètres aléatoires puis optimisé sur des milliards de jetons.</li>
        </ul>
        <h3>Fine-tuning (réglage fin)</h3>
        <p>Le modèle est ensuite adapté à des tâches spécifiques (ex. : résumé, Q&R, classification) via un ensemble de données plus petit et plus ciblé. Cela améliore sa performance sur des cas concrets.</p>
        <ul>
          <li>Approche : Supervised Fine-Tuning (SFT)</li>
          <li>Données : paires question-réponse, exemples annotés</li>
          <li>Exemple : InstructGPT</li>
        </ul>
        <h3>RLHF (Reinforcement Learning with Human Feedback)</h3>
        <p>Pour rendre les réponses plus utiles, le LLM est affiné via l’apprentissage par renforcement basé sur des préférences humaines. Il apprend à classer et améliorer ses propres sorties.</p>
        <ul>
          <li>Phase 1 : Collecte de préférences humaines</li>
          <li>Phase 2 : Apprentissage d’un modèle de récompense</li>
          <li>Phase 3 : Optimisation par PPO (Policy Optimization)</li>
        </ul>
        <h3>Déploiement</h3>
        <p>Une fois entraîné, le modèle est déployé via une API, sur le cloud ou localement. Il peut être intégré à des applications (chatbots, assistants, systèmes de recommandation...)</p>
        <h3>Techniques complémentaires</h3>
        <ul>
          <li><strong>Prompt engineering</strong> : formuler intelligemment les requêtes pour guider le comportement du modèle</li>
          <li><strong>RAG (Retrieval-Augmented Generation)</strong> : enrichir les prompts avec de la connaissance externe (base vectorielle)</li>
          <li><strong>Continual Pretraining</strong> : affiner un modèle avec de nouvelles données non annotées</li>
        </ul>
        <h3>Tableau comparatif des approches</h3>
        <div style="overflow-x: auto;">
          <table>
            <thead>
              <tr>
                <th>Méthode</th>
                <th>But</th>
                <th>Données requises</th>
                <th>Avantages</th>
                <th>Limites</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Pré-entraînement</td>
                <td>Modèle généraliste</td>
                <td>Corpus massif (non annoté)</td>
                <td>Apprentissage profond des patterns</td>
                <td>Coûteux, long, demande GPU</td>
              </tr>
              <tr>
                <td>Fine-tuning</td>
                <td>Spécialisation</td>
                <td>Corpus annoté (petit)</td>
                <td>Haute performance sur tâche précise</td>
                <td>Peut biaiser le modèle</td>
              </tr>
              <tr>
                <td>RLHF</td>
                <td>Alignement avec l'humain</td>
                <td>Retours humains</td>
                <td>Réponses plus utiles</td>
                <td>Complexité d’implémentation</td>
              </tr>
              <tr>
                <td>Prompt Engineering</td>
                <td>Optimisation à la volée</td>
                <td>Aucune</td>
                <td>Simple, rapide</td>
                <td>Moins robuste</td>
              </tr>
              <tr>
                <td>RAG</td>
                <td>Connaissances dynamiques</td>
                <td>Base externe (vectorielle)</td>
                <td>Informations à jour</td>
                <td>Coût d’inférence</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Ces étapes combinées permettent aux LLM d’être à la fois puissants, adaptables et efficaces pour une multitude d’applications.</p>
      </div>
    </section>

    <!-- Section 4: Optimisation -->
    <section id="optimisation">
      <div class="card">
        <h2>4. Optimisation des LLMs : Méthodes Avancées</h2>
        <p>Les grands modèles de langage nécessitent des techniques avancées pour réduire leur coût d'entraînement et d'inférence tout en maintenant leur efficacité. Voici deux méthodes majeures :</p>
        
        <h3>🔹 LoRA (Low-Rank Adaptation)</h3>
        <p>LoRA est une approche qui permet d’adapter un modèle existant en gelant ses poids d'origine et en n'entraînant que des matrices de faible rang supplémentaires. Cela réduit significativement le nombre de paramètres ajustés.</p>
        <p>LoRA repose sur une factorisation de matrice où une matrice de poids pré-entraînée <code>W</code> est décomposée en deux matrices de plus faible rang <code>A</code> et <code>B</code> telles que :</p>
        <p style="text-align: center; font-style: italic; font-size: 1.2rem;">ΔW = A × B<sup>T</sup></p>
        <p>Cela permet de minimiser le nombre de paramètres ajustés tout en conservant la richesse du modèle original.</p>
        <ul>
          <li><strong>Avantages :</strong> Réduction du coût de calcul et de mémoire.</li>
          <li><strong>Limites :</strong> Moins performant pour certaines tâches complexes nécessitant une mise à jour globale des poids.</li>
        </ul>
        <img src="27.png" alt="Illustration de la méthode LoRA">
        
        <h3>🔹 MoE (Mixture of Experts)</h3>
        <p>MoE repose sur l'activation sélective de sous-modèles appelés "experts" pour traiter différentes parties d’une requête, ce qui permet d'utiliser de grands modèles tout en limitant la charge de calcul.</p>
        <p>Dans MoE, un mécanisme de gating <code>G(x)</code> sélectionne dynamiquement un sous-ensemble d'experts <code>E<sub>i</sub></code>, chaque expert ayant son propre ensemble de paramètres <code>W<sub>i</sub></code> :</p>
        <p style="text-align: center; font-style: italic; font-size: 1.2rem;">y = ∑<sub>i=1</sub><sup>N</sup> G<sub>i</sub>(x) × E<sub>i</sub>(x)</p>
        <p>où <code>G<sub>i</sub>(x)</code> est une fonction d'activation qui attribue un poids à chaque expert.</p>
        <ul>
          <li><strong>Avantages :</strong> Scalabilité accrue et flexibilité.</li>
          <li><strong>Limites :</strong> Complexité accrue dans la gestion des experts et latence potentielle due au routage dynamique.</li>
        </ul>
        <img src="28.png" alt="Schéma du Mixture of Experts">
        
        <h3>Comparaison des méthodes</h3>
        <table>
          <thead>
            <tr>
              <th>Méthode</th>
              <th>Objectif</th>
              <th>Avantages</th>
              <th>Limites</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>LoRA</td>
              <td>Fine-tuning rapide et peu coûteux</td>
              <td>Faible consommation de mémoire</td>
              <td>Moins adapté aux tâches nécessitant une refonte du modèle</td>
            </tr>
            <tr>
              <td>MoE</td>
              <td>Modularité et gestion de grands modèles</td>
              <td>Scalabilité et optimisation de la charge</td>
              <td>Complexité de gestion et latence possible</td>
            </tr>
          </tbody>
        </table>
        <p>Ces deux approches sont complémentaires et peuvent être combinées pour améliorer l'efficacité des modèles de langage.</p>
      </div>
    </section>

    <!-- Section 5: Tendances -->
    <section id="tendances">
      <div class="card">
        <h2>5. Tendances Actuelles et Comparaison des Modèles de Langage</h2>
        <p>Les grands modèles de langage (LLM) évoluent rapidement, avec des avancées en matière d'efficacité, de scalabilité et de spécialisation. Voici quelques tendances majeures :</p>
        
        <h3>🔹 Tendance 1 : Modèles Open-Source vs Propriétaires</h3>
        <p>Les modèles de langage se divisent aujourd’hui en deux grandes catégories :</p>
        <ul>
          <li><strong>Modèles propriétaires</strong> : Développés par des entreprises comme OpenAI (GPT-4), Google DeepMind (Gemini), ou Anthropic (Claude), ces modèles sont généralement fermés et optimisés pour des performances de pointe. Toutefois, leur opacité en matière d'entraînement et de fonctionnement pose des défis en termes de transparence et de contrôle.</li>
          <li><strong>Modèles open-source</strong> : Des alternatives comme LLaMA 2 (Meta), Falcon (Technology Innovation Institute) et DeepSeek (DeepSeek AI) permettent aux chercheurs et ingénieurs de les modifier et de les adapter à des besoins spécifiques, favorisant ainsi une plus grande transparence et une adoption plus large.</li>
        </ul>
        
        <h3>🔹 Tendance 2 : Efficacité et Réduction de la Consommation</h3>
        <p>Avec l’augmentation exponentielle de la taille des modèles, la question de l’efficacité devient centrale. Plusieurs approches sont utilisées pour optimiser la consommation énergétique et le coût :</p>
        <ul>
          <li><strong>Quantization</strong> : Réduction de la précision des poids (ex. FP16, INT8) pour accélérer l’inférence et réduire la consommation de mémoire.</li>
          <li><strong>Distillation</strong> : Extraction des connaissances d’un modèle volumineux pour entraîner un modèle plus léger sans perte significative de performance.</li>
          <li><strong>Fine-tuning efficace</strong> : Approches comme LoRA et AdapterFusion permettent d’adapter un modèle à de nouvelles tâches sans réentraînement complet.</li>
        </ul>
        
        <h3>🔹 Tendance 3 : Spécialisation et Fine-Tuning</h3>
        <p>Plutôt que d'entraîner des modèles généralistes, l'industrie se tourne vers des modèles spécialisés :</p>
        <ul>
          <li><strong>BioGPT</strong> : Dédié aux applications biomédicales et à la recherche médicale.</li>
          <li><strong>Codex</strong> : Conçu pour la génération et la complétion de code informatique.</li>
          <li><strong>BloombergGPT</strong> : Axé sur les domaines de la finance et de l'analyse économique.</li>
          <li><strong>DeepSeek</strong> : Un modèle open-source puissant qui se concentre sur la compréhension et la génération de texte en plusieurs langues, notamment en chinois et en anglais.</li>
        </ul>
        
        <p>La course à l’innovation dans le domaine des LLMs ne cesse d’accélérer, avec une attention croissante portée à l'efficacité, la spécialisation et l’accessibilité des modèles.</p>
      </div>
    </section>

    <!-- Section 6: Outils de Veille -->
    <section id="outils">
      <div class="card">
        <h2>6. Outils de Veille Utilisés</h2>
        <h3>Recherche et Collecte d'Informations</h3>
        <ul>
          <li><strong>Google Scholar</strong> : Moteur de recherche académique permettant d'accéder aux articles scientifiques et brevets.</li>
          <li><strong>Arxiv.org &amp; Arxiv-Sanity</strong> : Plateforme incontournable pour les prépublications en intelligence artificielle et machine learning.</li>
          <li><strong>Semantic Scholar</strong> : Outil avancé d'indexation des articles scientifiques avec des fonctionnalités d'analyse sémantique.</li>
        </ul>
        <h3>Organisation et Gestion des Sources</h3>
        <ul>
          <li><strong>Zotero</strong> : Outil de gestion bibliographique permettant de stocker, organiser et citer des articles.</li>
          <li><strong>Mendeley</strong> : Alternative à Zotero pour la gestion et l'annotation de références scientifiques.</li>
        </ul>
        <h3>📡 Veille en Temps Réel</h3>
        <ul>
          <li><strong>Feedly</strong> : Agrégateur de flux RSS permettant de suivre les publications récentes.</li>
          <li><strong>Twitter/X</strong> : Réseau social où chercheurs et laboratoires publient des avancées en temps réel.</li>
          <li><strong>LinkedIn</strong> : Plateforme professionnelle utile pour suivre les innovations et conférences en IA.</li>
          <li><strong>Hugging Face Forums</strong> : Communauté dédiée aux LLMs, avec des discussions sur les derniers modèles et optimisations.</li>
          <li><strong>PapersWithCode</strong> : Base de données associant articles scientifiques et implémentations open-source.</li>
        </ul>
      </div>
    </section>

    <!-- Section 7: Ressources Bibliographiques -->
    <section id="bibliographie">
      <div class="card">
        <h2>7. Ressources Bibliographiques</h2>
        <p>Les références suivantes regroupent les principales publications scientifiques utilisées dans cette veille technologique :</p>
        <h3>📄 Articles Scientifiques</h3>
        <ul>
          <li><a href="https://arxiv.org/abs/2106.09685" target="_blank">LoRA: Low-Rank Adaptation of Large Language Models</a> - Hu et al., 2021</li>
          <li><a href="https://arxiv.org/abs/2406.03136" target="_blank">Limitations of LoRA: A Comprehensive Analysis</a> - Étude des contraintes de LoRA</li>
          <li><a href="https://openreview.net/forum?id=B1ckMDqlg" target="_blank">Mixture of Experts in Transformers</a> - Shazeer et al., 2017</li>
          <li><a href="https://arxiv.org/abs/2202.00580" target="_blank">Scaling Laws for Neural Language Models</a> - Analyse des relations entre taille du modèle et performance</li>
        </ul>
        <h3>📚 Plateformes et Outils</h3>
        <ul>
          <li><a href="https://huggingface.co" target="_blank">Hugging Face</a> : Hub collaboratif pour le développement de modèles IA</li>
          <li><a href="https://paperswithcode.com" target="_blank">Papers with Code</a> : Suivi des dernières avancées en IA avec implémentations disponibles</li>
          <li><a href="https://www.semanticscholar.org" target="_blank">Semantic Scholar</a> : Base de données scientifique avec indexation avancée</li>
        </ul>
      </div>
    </section>
  </main>

  <footer>
    <p>© 2025 Mehdi Janati Idrissi — Projet de Veille Technologique Centrale Lyon</p>
  </footer>

  <!-- Script pour le scroll fluide et la mise en surbrillance de la section active -->
  <script>
    const navLinks = document.querySelectorAll('.nav-link');

    // Supprime la classe active de tous les liens
    function removeActiveClasses() {
      navLinks.forEach(link => link.classList.remove('active'));
    }

    // Ajoute la classe active lors d'un clic sur un lien
    navLinks.forEach(link => {
      link.addEventListener('click', function() {
        removeActiveClasses();
        this.classList.add('active');
      });
    });

    // Met à jour le lien actif en fonction de la position de scroll
    window.addEventListener('scroll', () => {
      let fromTop = window.scrollY + 100;
      navLinks.forEach(link => {
        let section = document.querySelector(link.getAttribute('href'));
        if (
          section.offsetTop <= fromTop &&
          section.offsetTop + section.offsetHeight > fromTop
        ) {
          removeActiveClasses();
          link.classList.add('active');
        }
      });
    });
  </script>
</body>
</html>
